{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "    https://github.com/mattnedrich/GradientDescentExample/blob/master/gradient_descent_example.gif\n",
    "    \n",
    "The example above shows how linear regression and gradient descent combined can fit a line to data. \n",
    "\n",
    "We start with a random linear function, compute the error, that error value tells us how well the line fits and acts as a compass for redrawing another line. Once we get the \"Best\" line we finish. \n",
    "\n",
    "Below is an example of coding this from scratch, using the error function:\n",
    "https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png\n",
    "\n",
    "visual for gradient descent error: \n",
    "https://spin.atomicobject.com/wp-content/uploads/gradient_descent_error_surface.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing gradient descent at b = 0, m = 0 error=5565.10783448\n",
      "final values are: b = 1000, m = -9.45778571658e+173 error=-4.80825345907e+175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:12: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# this function is the code for the above error function\n",
    "def error_compute(b, m, points):\n",
    "    total_error = 0\n",
    "    # iterate through points \n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        \n",
    "        # compute the difference and square for error, then add to total error: \n",
    "        total_error += (y - (m*x+b))**2\n",
    "    return total_error/float(len(points)) # average error\n",
    "\n",
    "\n",
    "\n",
    "def grad_step(b_current, m_current, points, learning_rate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    n = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # direction with respect to b and m\n",
    "        # compute partial derivatives of error function\n",
    "        b_gradient += (2/n)*(y - ((m_current*x) + b_current))\n",
    "        m_gradient += (2/n)*x*(y-(m_current*x) + b_current)\n",
    "        \n",
    "    #update b and m values using partial derivatives\n",
    "    new_b = b_current - (learning_rate*b_gradient)\n",
    "    new_m = m_current - learning_rate*m_gradient\n",
    "    \n",
    "    return [new_b, new_m]\n",
    "\n",
    "\n",
    "# gradient descent function:\n",
    "def grad_descent(points, starting_b, starting_m, learning_rate, n_iter):\n",
    "    b = starting_b \n",
    "    m = starting_m \n",
    "    \n",
    "    # grad descent\n",
    "    for i in range(n_iter):\n",
    "        #update b and m in a step\n",
    "        b, m = grad_step(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def run():\n",
    "    # step 1: data\n",
    "    points = genfromtxt('data.csv', delimiter=',') # delimiter is argument\n",
    "    # for how data values are separated \n",
    "    \n",
    "    # step 2: the parameters for the model\n",
    "    learning_rate = .0001 # defines how fast the model should converge\n",
    "    initial_b = 0   # inital slope and y-intercept for y = mx+b fit\n",
    "    initial_m = 0\n",
    "    n_iter = 1000 # since data is small this can be small \n",
    "    \n",
    "    # step 3: train the model \n",
    "    print 'initializing gradient descent at b = {0}, m = {1} error={2}'.format(initial_b,\n",
    "                                                                     initial_m,\n",
    "                                                                     error_compute(initial_b, initial_m, points))\n",
    "    \n",
    "\n",
    "    [b, m] = grad_descent(points, initial_b, initial_m, learning_rate, n_iter)\n",
    "    \n",
    "    print 'final values are: b = {0}, m = {1} error={2}'.format(n_iter, b,\n",
    "                                                                     m,\n",
    "                                                                     error_compute(b, m, points))\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
